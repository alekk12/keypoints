{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cfdc788",
   "metadata": {},
   "source": [
    "The goal of the project is to simultaneously predict keypoints and age of the person in the image. This can be later combined with bounding box detection of a person or having multiple instances in the heatmaps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78452e7",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5911603",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b485db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "import torchvision.io as io\n",
    "import torch.nn.functional as F \n",
    "from PIL import Image\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597fe4e2",
   "metadata": {},
   "source": [
    "### Const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ede8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_KEYPOINTS = 17\n",
    "NUM_AGE_CLASSES = 2 #adult vs child, adult(1), child(0), infant in the future (1)\n",
    "IMAGE_SIZE = (256, 192)\n",
    "HEATMAP_SIZE = (8,6)#(64, 48)\n",
    "B = 1\n",
    "C = 2 #how important is age to the loss\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0796b7",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57632c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_2d(shape:tuple, center:tuple, sigma=2)->torch.Tensor:\n",
    "    \"\"\"Gaussian formula for smoothing/blurring\n",
    "    if sigma_x==sigma_y the shape is circular (keypoint), otherwise ellipsis\n",
    "\n",
    "    Args:\n",
    "        shape (tuple): heatmap size (h,w)\n",
    "        center (tuple): coordinates (x,y)\n",
    "        sigma (int, optional): parameter Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: heatmap\n",
    "    \"\"\"\n",
    "    h, w = shape\n",
    "    y = torch.arange(0,h).float()\n",
    "    x = torch.arange(0,w).float()\n",
    "    yy, xx = torch.meshgrid(y,x,indexing=\"ij\")\n",
    "    x0, y0 = center\n",
    "    return torch.exp(-((xx-x0)**2+(yy-y0)**2)/(2*sigma**2))\n",
    "\n",
    "def kps_to_heatmaps(kps:torch.Tensor, nr_kps:int = NUM_KEYPOINTS, ht_size:int = HEATMAP_SIZE,th=0)-> dict:\n",
    "    \"\"\"Converts keypoints of one person to a dictionary of heatmaps (one per keypoint)\n",
    "\n",
    "    Args:\n",
    "        kps (torch.Tensor): person's keypoints NUM_KEYPOINTS*3\n",
    "        nr_kps (int, optional): number of keypoints. Defaults to NUM_KEYPOINTS.\n",
    "        ht_size (int, optional): size of the heatmap. Defaults to HEATMAP_SIZE.\n",
    "        th (int, optional): threshold when keypoints are considered. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of heatmaps, with one per keypoint (x,y)\n",
    "    \"\"\"\n",
    "    ht = torch.zeros(nr_kps, *ht_size)\n",
    "\n",
    "    for k in range(nr_kps):\n",
    "        x, y, _ = kps[k*3:k*3+3]\n",
    "        x = x * ht_size[1]\n",
    "        y = y * ht_size[0]\n",
    "\n",
    "        ht[k] = gaussian_2d(\n",
    "            ht_size, center=(x,y),sigma=2\n",
    "        )\n",
    "    return ht\n",
    "\n",
    "def one_person_heatmaps_to_kps(hts:dict,img_size:tuple=IMAGE_SIZE, ht_size:tuple=HEATMAP_SIZE, th:int=0)->list:\n",
    "    \"\"\"Converts heatmaps of one person to keypoints\n",
    "       Each keypoint has (x,y,conf) where conf=2 means it is visible, conf=0 it was not detected\n",
    "\n",
    "    Args:\n",
    "        hts (dict): heatmaps\n",
    "        img_size (tuple, optional): size of the image. Defaults to IMAGE_SIZE.\n",
    "        ht_size (tuple, optional): _size/dimensions of the image. Defaults to HEATMAP_SIZE.\n",
    "        th (int, optional):threshold. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        list: list of keypoints\n",
    "    \"\"\"\n",
    "    k_nr, ht_h, ht_w = hts.shape\n",
    "    img_w, img_h = img_size\n",
    "    kps = []\n",
    "    for k in range(k_nr):\n",
    "        temp = hts[k]\n",
    "        if temp.max() <= th:\n",
    "            kps.extend([0,0,0])\n",
    "            continue\n",
    "        #resize\n",
    "        idx = temp.argmax()\n",
    "        x_img = (idx.item() % ht_w) * img_w/ht_w\n",
    "        y_img = (idx.item() // ht_w) * img_h/ht_h\n",
    "        kps.extend([x_img,y_img,2])\n",
    "    return kps\n",
    "\n",
    "def people_heatmaps_to_kps(ht)->list:\n",
    "    \"\"\"Converts heatmaps of multiple people to keypoints\n",
    "\n",
    "    Args:\n",
    "        ht (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        list: list of keypoints\n",
    "    \"\"\"\n",
    "    N, _, _, _ = ht.shape\n",
    "    all_keypoints = []\n",
    "    for n in range(N):\n",
    "        all_keypoints.extend(one_person_heatmaps_to_kps(ht[n]))\n",
    "    return all_keypoints\n",
    "\n",
    "def ht_to_coord(ht, topk:int=17)->torch.Tensor:\n",
    "    \"\"\"Converts heatmap to coordinates. Softmax is used to normalize the heatmaps\n",
    "\n",
    "    Args:\n",
    "        ht (_type_): heatmap\n",
    "        topk (int, optional): _description_. Defaults to 17.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: coordinates\n",
    "    \"\"\"\n",
    "    N, C, H, W = ht.shape\n",
    "    score, index = ht.view(N,C,1,-1).topk(topk, dim=-1)\n",
    "    coord = torch.cat([index%W, index//H], dim=2)\n",
    "    return (coord*F.softmax(score, dim=-1)).sum(-1)\n",
    "\n",
    "def load_data(ann, label_keys, th:int)->list:\n",
    "    \"\"\"Loads data as a list of dictionaries\n",
    "    Removes duplicates - only images where one person is detected are used\n",
    "\n",
    "    Args:\n",
    "        ann (dict): annotations\n",
    "        label_keys (dict_keys): _description_\n",
    "        th (int):min number of keypoints\n",
    "\n",
    "    Returns:\n",
    "        list: data with annotations\n",
    "    \"\"\"\n",
    "    data = [a for a in ann if a['num_keypoints'] > th and a['image_id'] in label_keys]\n",
    "    counts = Counter(d['image_id'] for d in data)\n",
    "    data = [d for d in data if counts[d['image_id']]==1]\n",
    "    print(f\"Annotations:{len(data)}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75aa0a4",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12bd3766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputDataset(Dataset):\n",
    "    def __init__(self, ann_path:str, data_dir:str, label_path:str,th:int=0,is_gag:bool=False):\n",
    "        self.data_dir = data_dir\n",
    "        with open(ann_path, \"r\" ) as f:\n",
    "            ann = json.load(f)\n",
    "        self.labels = pd.read_csv(label_path).set_index('image_id').T.to_dict()\n",
    "        self.data = load_data(ann['annotations'], self.labels.keys(), th)\n",
    "        self.id_to_name_map = {\n",
    "            img['id'] : img['file_name']\n",
    "            for img in ann['images']\n",
    "        }\n",
    "        self.is_gag = is_gag\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize(IMAGE_SIZE),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485,0.456,0.406],\n",
    "                        std=[0.229,0.224,0.225])\n",
    "        ])\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        kp = self.data[index]\n",
    "        img_id = kp['image_id']\n",
    "        img_path = os.path.join(\n",
    "            self.data_dir,\n",
    "            self.id_to_name_map[img_id]\n",
    "        )\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        kps = torch.tensor(kp['keypoints']).float()\n",
    "        ht = kps_to_heatmaps(kps)\n",
    "        age = torch.tensor(int(self.labels[img_id]['age']=='adult'), dtype=torch.int64)        \n",
    "        return image, ht, age\n",
    "    \n",
    "\n",
    "class MixedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])#removes classification\n",
    "        self.pose_head = nn.Sequential(\n",
    "            nn.Conv2d(2048,256,3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256,NUM_KEYPOINTS,1)\n",
    "        )\n",
    "        self.age_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048,256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, NUM_AGE_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)\n",
    "        ht = self.pose_head(feat)\n",
    "        age = self.age_head(feat)\n",
    "        return ht, age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8963cf83",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce0383",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7566bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(model:MixedModel, epochs:int, loader:DataLoader, optimizer, scheduler, \n",
    "                 pose_criterion, age_criterion, device:str=DEVICE,\n",
    "                 path:str='./model_heatmap.pth', step_print:bool=True, \n",
    "                 th:int=0.25, B:int=1,C:int=2):\n",
    "    \"\"\"Train the model for `epochs`. Save model in the given `path`\n",
    "\n",
    "    Args:\n",
    "        model (MixedModel): _description_\n",
    "        epochs (int): _description_\n",
    "        loader (DataLoader): _description_\n",
    "        optimizer (_type_): _description_\n",
    "        scheduler (_type_): _description_\n",
    "        pose_criterion (_type_): _description_\n",
    "        age_criterion (_type_): _description_\n",
    "        device (str, optional): _description_. Defaults to DEVICE.\n",
    "        path (str, optional): _description_. Defaults to './model_heatmap.pth'.\n",
    "    \"\"\"\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        for images, gt_heatmaps, age_labels in loader:\n",
    "            optimizer.zero_grad()\n",
    "            images = images.to(device)\n",
    "            gt_heatmaps = gt_heatmaps.to(device)\n",
    "            pred_heatmaps, pred_age = model(images)\n",
    "            loss_pose = pose_criterion(pred_heatmaps, gt_heatmaps)\n",
    "            loss = loss_pose\n",
    "            if age_labels[0] is not None:\n",
    "                age_labels = age_labels.to(device)\n",
    "                loss_age = age_criterion(pred_age, age_labels)\n",
    "                loss = B * loss + C * loss_age            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        scheduler.step()\n",
    "        epoch_loss = round(total_loss / num_batches,4)\n",
    "        lr = round(scheduler.get_last_lr()[0],4)\n",
    "        print(f\"Epoch {e}: loss = {epoch_loss:.4f} lr = {lr}\")\n",
    "        if step_print and epoch_loss < th:\n",
    "            torch.save(model.state_dict(),f'./model_train_heatmap_{epoch_loss}loss_{e}_{epochs}ep_{lr}lr_Adam_OneCycleLR.pth')\n",
    "    torch.save(model.state_dict(), path)  \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c9516a",
   "metadata": {},
   "source": [
    "* MSELoss ranges from $0$ to $1$\n",
    "* CrossEntropyLoss (for two classes, `adult` and `child`) ranges from $0$ to $0.693$ \n",
    "  \n",
    "The final loss formula is `B * pose_loss + C * age_loss` where $C$ is a constant\n",
    "\n",
    "I decided to make $B$ equal to $1$ and $C$ equal to $2$ since age is really imporant in this model\n",
    "\n",
    "The final loss ranges from $0$ to $2.386$ with good values of loss being less than $0.24$ (scaled to 0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bd089fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_criterion = nn.MSELoss(reduction='mean') #normalize heatmap, good especially for large heatmaps\n",
    "age_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d02b5b",
   "metadata": {},
   "source": [
    "* `keypoints_path` - json from coco website with information. It has `annotations` where each annotation has the `keypoints`, `num_keypoints`, `image_id`, `id` and `images` with `filename` and other information about the images.\n",
    "* `img_dir` - directory with jpg images from `keypoints_path` json\n",
    "* `label_path` - path to a csv. The ages were predicted using both the body ratios (MMU gag dataset) and captions from coco dataset, then manually verified by looking through images. Csv has three columns `age` (child|adult), `image_id` (filename), `id`\n",
    "* `lr` - learning rate chosen using optuna after adding the scheduler\n",
    "* `epochs` - number of epochs chosen using optuna + verified by saving each model after certain number of epochs\n",
    "* `opt` - Adam optimizer used for training\n",
    "* `sch` - OneCycleLR used as a scheduler, the learning rate is annealed until we reach maximum and then we decrease the learning rate lower than the initial learning rate `lr`\n",
    "* `heatmap` - this version of the model uses heatmaps while predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3213dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations:3096\n"
     ]
    }
   ],
   "source": [
    "is_train = \"train\"\n",
    "keypoints_path = f\"./data/person_keypoints_{is_train}2017.json\"\n",
    "img_dir = f'./data/train2017_man_{is_train}'\n",
    "label_path = f'./data/label_coco_man_{is_train}.csv'\n",
    "\n",
    "lr = 0.005\n",
    "epochs = 10\n",
    "opt = \"Adam\"\n",
    "sch = \"OneCycleLR\"\n",
    "ht = \"heatmap\"\n",
    "\n",
    "path = f'./models/model_{is_train}_{ht}_lr{lr}_ep{epochs}_opt{opt}_sch{sch}.pth'\n",
    "\n",
    "train_data = InputDataset(keypoints_path,img_dir, label_path)\n",
    "train_loader = DataLoader(train_data, batch_size=64, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b280b9a",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527012b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4530417919158936\n",
      "2.078248620033264\n",
      "1.8649099667867024\n",
      "1.7049901187419891\n",
      "1.5985094070434571\n",
      "1.5312675038973491\n",
      "1.4686125005994524\n",
      "1.4135803878307343\n",
      "1.3561420573128595\n",
      "1.3199161469936371\n",
      "1.2806583480401472\n",
      "1.274994785586993\n",
      "1.2370428718053377\n",
      "1.2060591280460358\n",
      "1.1879186550776164\n",
      "1.176233023405075\n",
      "1.1572419860783745\n",
      "1.155966450770696\n",
      "1.1332629103409617\n",
      "1.1172718584537507\n",
      "1.1051660123325528\n",
      "1.085524702614004\n",
      "1.0685441053431968\n",
      "1.067219781378905\n",
      "1.0663857913017274\n",
      "1.061854185966345\n",
      "1.0475118822521634\n",
      "1.0410564371517725\n",
      "1.037568410922741\n",
      "1.0275382816791534\n",
      "1.0220191267228895\n",
      "1.0078760907053947\n",
      "1.0002230261311387\n",
      "0.9916905497803408\n",
      "0.9799893702779497\n",
      "0.977400971783532\n",
      "0.9774602619377343\n",
      "0.9679893270919198\n",
      "0.9608410719113473\n",
      "0.9586742758750916\n",
      "0.955235761840169\n",
      "0.9466909993262518\n",
      "0.9433327885561211\n",
      "0.940576508641243\n",
      "0.9370939254760742\n",
      "0.936847518319669\n",
      "0.9347328340753596\n",
      "0.9320910212894281\n",
      "0.9258451157686661\n",
      "Epoch 0: loss = 0.9258 lr = 0.0002\n",
      "0.3453093469142914\n",
      "0.41063089668750763\n",
      "0.3763018548488617\n",
      "0.39257605373859406\n",
      "0.3962224543094635\n",
      "0.39583760996659595\n",
      "0.4035038650035858\n",
      "0.4025842510163784\n",
      "0.40671856535805595\n",
      "0.39478660821914674\n",
      "0.3935761641372334\n",
      "0.41232652217149734\n",
      "0.4206247856983772\n",
      "0.4268172319446291\n",
      "0.4204597691694895\n",
      "0.41952574998140335\n",
      "0.41651762583676505\n",
      "0.41389979587660897\n",
      "0.41159250077448395\n",
      "0.4056739956140518\n",
      "0.40698608330317904\n",
      "0.4085215506228534\n",
      "0.4134247601032257\n",
      "0.41123438129822415\n",
      "0.4106763780117035\n",
      "0.40930605966311234\n",
      "0.41862087448438007\n",
      "0.42145532050303053\n",
      "0.42024373802645454\n",
      "0.41929783423741657\n",
      "0.4134405625443305\n",
      "0.4097836413420737\n",
      "0.4067851300492431\n",
      "0.4121601559660014\n",
      "0.4147527094398226\n",
      "0.4162708856165409\n",
      "0.4162191728482375\n",
      "0.41593476149596664\n",
      "0.41449092710629487\n",
      "0.4130867924541235\n",
      "0.4114468341193548\n",
      "0.41126722487665357\n",
      "0.4189892768166786\n",
      "0.4177190644497221\n",
      "0.4183073225948546\n",
      "0.4167558301402175\n",
      "0.4133038822006672\n",
      "0.4111699868614475\n",
      "0.41298486109899013\n",
      "Epoch 1: loss = 0.4130 lr = 0.0002\n",
      "0.15391764044761658\n",
      "0.15221820026636124\n",
      "0.1560332179069519\n",
      "0.16333526000380516\n",
      "0.17047998309135437\n",
      "0.17850332955519357\n",
      "0.17670084961823054\n",
      "0.1883577834814787\n",
      "0.18559817969799042\n",
      "0.1854092910885811\n",
      "0.18777656148780475\n",
      "0.18402515724301338\n",
      "0.18036196782038763\n",
      "0.17602104533995902\n",
      "0.17562676618496578\n",
      "0.17321934876963496\n",
      "0.1713982921312837\n",
      "0.1684486505885919\n",
      "0.17410193344480113\n",
      "0.17295319698750972\n",
      "0.17945577097790583\n",
      "0.17705787142569368\n",
      "0.17928476016158643\n",
      "0.18000232086827359\n",
      "0.18188035994768142\n",
      "0.18109823963963068\n",
      "0.17990983222369794\n",
      "0.17996115955923284\n",
      "0.18263612610512767\n",
      "0.18514832034707068\n",
      "0.18748968719474732\n",
      "0.18491559848189354\n",
      "0.18377076631242578\n",
      "0.1886645200497964\n",
      "0.18924926349094937\n",
      "0.19123674598005083\n",
      "0.19347455936509209\n",
      "0.1942444224106638\n",
      "0.19243173148387518\n",
      "0.19621493853628635\n",
      "0.19796919859037165\n",
      "0.19889575562306813\n",
      "0.19908535861691765\n",
      "0.19779664718291975\n",
      "0.19754056500064002\n",
      "0.20564728165450302\n",
      "0.20581726539642253\n",
      "0.20741473169376454\n",
      "0.20768867281018472\n",
      "Epoch 2: loss = 0.2077 lr = 0.0002\n",
      "0.3285466730594635\n",
      "0.21703798696398735\n",
      "0.23646216342846552\n",
      "0.22077160514891148\n",
      "0.19616083055734634\n",
      "0.18477127576867738\n",
      "0.18274285005671637\n",
      "0.20420902501791716\n",
      "0.20582772377464506\n",
      "0.20757125839591026\n",
      "0.20086964761668985\n",
      "0.1960719283670187\n",
      "0.19604176569443482\n",
      "0.19422415590712003\n",
      "0.20989909023046494\n",
      "0.20990140968933702\n",
      "0.20521135584396474\n",
      "0.2156247782210509\n",
      "0.2131233901569718\n",
      "0.21149667613208295\n",
      "0.2086439718093191\n",
      "0.21053802730007606\n",
      "0.20842447714961093\n",
      "0.20462315063923597\n",
      "0.2071522918343544\n",
      "0.21068516058417466\n",
      "0.21040096741031716\n",
      "0.2098432265754257\n",
      "0.2115341617629446\n",
      "0.20987854773799577\n",
      "0.21610147409862088\n",
      "0.21311335382051766\n",
      "0.2121946642344648\n",
      "0.21043706137467832\n",
      "0.2084700650402478\n",
      "0.20669916292859447\n",
      "0.2063969037420041\n",
      "0.20505054549951302\n",
      "0.20418498474053848\n",
      "0.20353270489722491\n",
      "0.2040556780085331\n",
      "0.2035488211328075\n",
      "0.20292938188758008\n",
      "0.2033664115111936\n",
      "0.20203494048780865\n",
      "0.20014439895749092\n",
      "0.20034196291198123\n",
      "0.20260280448322496\n",
      "0.2012299203446933\n",
      "Epoch 3: loss = 0.2012 lr = 0.0003\n",
      "0.07836414873600006\n",
      "0.11745242029428482\n",
      "0.11657412350177765\n",
      "0.12086402997374535\n",
      "0.11233403384685517\n",
      "0.12479548901319504\n",
      "0.1372797233717782\n",
      "0.15184038132429123\n",
      "0.14996901651223501\n",
      "0.14515408501029015\n",
      "0.15107682821425525\n",
      "0.14721085814138254\n",
      "0.1563734681560443\n",
      "0.15046130865812302\n",
      "0.1499080260594686\n",
      "0.14758123783394694\n",
      "0.15335251697722604\n",
      "0.15474030334088537\n",
      "0.1507626720949223\n",
      "0.15157443694770337\n",
      "0.15082872871841704\n",
      "0.14960037408904595\n",
      "0.15756299152322437\n",
      "0.1619520979002118\n",
      "0.16201501697301865\n",
      "0.16664673244723907\n",
      "0.16629860550165176\n",
      "0.1639502884021827\n",
      "0.1665138823205027\n",
      "0.16823025743166606\n",
      "0.1671246915094314\n",
      "0.1709428485482931\n",
      "0.1714352902137872\n",
      "0.169909148970071\n",
      "0.170740139910153\n",
      "0.16983991033501095\n",
      "0.1703874807100038\n",
      "0.1700392298792538\n",
      "0.1737889467905729\n",
      "0.1715815771371126\n",
      "0.17055555669272818\n",
      "0.17429037534055256\n",
      "0.1776865589064221\n",
      "0.1758930351246487\n",
      "0.18034590284029642\n",
      "0.18220073179058408\n",
      "0.1856759215923066\n",
      "0.18816850458582243\n",
      "0.18607637879191613\n",
      "Epoch 4: loss = 0.1861 lr = 0.0003\n",
      "0.19694307446479797\n",
      "0.20369602739810944\n",
      "0.17940532167752585\n",
      "0.18042726442217827\n",
      "0.17462277710437774\n",
      "0.17904865990082422\n",
      "0.20276791070188796\n",
      "0.1974265854805708\n",
      "0.19312223295370737\n",
      "0.2000231549143791\n",
      "0.19396456669677387\n",
      "0.21256196374694505\n",
      "0.2123782623272676\n",
      "0.20760671262230193\n",
      "0.20198175261418025\n",
      "0.20837714476510882\n",
      "0.2046177137423964\n",
      "0.2084721140563488\n",
      "0.21558323933889992\n",
      "0.21996989510953427\n",
      "0.21806832119112923\n",
      "0.22248358448798006\n",
      "0.22379789631003918\n",
      "0.22186509302506843\n",
      "0.22139618068933486\n",
      "0.22344273586685842\n",
      "0.22659527400025614\n",
      "0.22704963412668025\n",
      "0.22750229491242047\n",
      "0.22912568474809328\n",
      "0.2266500224509547\n",
      "0.22159801865927875\n",
      "0.22415820825280566\n",
      "0.22459186372511528\n",
      "0.22231480883700508\n",
      "0.22569877054128382\n",
      "0.22489335065757907\n",
      "0.22249454044197736\n",
      "0.21915845763988984\n",
      "0.2186301827430725\n",
      "0.22435674216689133\n",
      "0.22733943732011885\n",
      "0.2301790679610053\n",
      "0.23444302176887338\n",
      "0.23797028329637315\n",
      "0.2365607781254727\n",
      "0.23797009219514562\n",
      "0.2419652839501699\n",
      "0.2427076265519979\n",
      "Epoch 5: loss = 0.2427 lr = 0.0004\n",
      "0.13502827286720276\n",
      "0.14780541509389877\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect() #clears cache bcs sometimes it breaks\n",
    "    \n",
    "    model = MixedModel().to(DEVICE)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                max_lr = lr,\n",
    "                                                steps_per_epoch=len(train_loader),\n",
    "                                                epochs=epochs,\n",
    "                                                pct_start=0.1,\n",
    "                                                anneal_strategy='cos'\n",
    "                                                )\n",
    "\n",
    "    for p in model.age_head.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    epoch_loss = train_epochs(model, epochs, train_loader, optimizer,scheduler, pose_criterion, age_criterion, path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c9f1d",
   "metadata": {},
   "source": [
    "### Optuna study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb86383b",
   "metadata": {},
   "source": [
    "#### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a384776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import accuracy_score\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "def objective(trial)->float:\n",
    "    \"\"\"Runs each trial\n",
    "\n",
    "    Args:\n",
    "        trial (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        float: Returns loss when training\n",
    "    \"\"\"\n",
    "    model = MixedModel().to(DEVICE)\n",
    "    is_train = \"train\"\n",
    "    keypoints_path = f\"./data/person_keypoints_{is_train}2017.json\"\n",
    "    img_dir = f'./data/train2017_man_{is_train}'\n",
    "    label_path = f'./data/label_coco_man_{is_train}.csv'\n",
    "    train_data = InputDataset(keypoints_path,img_dir, label_path)\n",
    "    train_loader = DataLoader(train_data, batch_size=64, num_workers=0, shuffle=True)\n",
    "    pose_criterion = nn.MSELoss(reduction='mean') #normalize heatmap, good especially for large heatmaps\n",
    "    age_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for p in model.age_head.parameters():\n",
    "        p.requires_grad = False\n",
    "    epochs = trial.suggest_int(\"epochs\",0,100)\n",
    "    lr = trial.suggest_int(\"lr\",1e-3, 1e-1)\n",
    "    pct_start = trial.suggest_float(\"pct_start\",0.05, 0.3)\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                max_lr = lr,\n",
    "                                                steps_per_epoch=len(train_loader),\n",
    "                                                epochs=epochs,\n",
    "                                                pct_start=pct_start,\n",
    "                                                anneal_strategy='cos'\n",
    "                                                )\n",
    "    return train_epochs(model, epochs, train_loader, optimizer, scheduler, pose_criterion, age_criterion, step_print=False)\n",
    "    # model.fit(X_train, y_train)\n",
    "    # y_pred = model.predict(X_test)\n",
    "    # return accuracy_score(y_test, y_pred)\n",
    "    #TO DO\n",
    "    # add accuracy of age and separately for heatmaps/keypoints\n",
    "\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "sampler = TPESampler(seed=1)\n",
    "\n",
    "study = optuna.create_study(study_name=\"coco\", direction=\"minimize\", sampler=sampler)\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5819c05b",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47beef5c",
   "metadata": {},
   "source": [
    "Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccaa606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(model:MixedModel, loader:DataLoader,\n",
    "                 pose_criterion, age_criterion, device:str=DEVICE,\n",
    "                 B:int=1, C:int=2)->tuple[list,list]:\n",
    "    \"\"\" Predict for all data in the test loader\n",
    "\n",
    "    Args:\n",
    "        model (MixedModel): _description_\n",
    "        epochs (int): _description_\n",
    "        loader (DataLoader): _description_\n",
    "        pose_criterion (_type_): _description_\n",
    "        age_criterion (_type_): _description_\n",
    "        device (str, optional): _description_. Defaults to DEVICE.\n",
    "        path (str, optional): _description_. Defaults to './model_heatmap.pth'.   \n",
    "    Returns:\n",
    "        tuple[list,list]: returns two lists of keypoints, one extracted normally and one softmaxed\n",
    "    \"\"\"\n",
    "    kps = []\n",
    "    kps2 = []\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, gt_heatmaps, age_labels in loader:\n",
    "                images = images.to(device)\n",
    "                gt_heatmaps = gt_heatmaps.to(device)\n",
    "                pred_heatmaps, pred_age = model(images)\n",
    "                kps.extend(people_heatmaps_to_kps(pred_heatmaps))\n",
    "                kps2.extend(ht_to_coord(pred_heatmaps))\n",
    "                loss_pose = pose_criterion(pred_heatmaps, gt_heatmaps)\n",
    "                loss = loss_pose\n",
    "                if age_labels[0] is not None:\n",
    "                    age_labels = age_labels.to(DEVICE)\n",
    "                    loss_age = age_criterion(pred_age, age_labels)\n",
    "                    loss = B * loss + C * loss_age\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "        print(f\"Testing loss = {total_loss / num_batches:.4f}\")\n",
    "    return kps, kps2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65149cc8",
   "metadata": {},
   "source": [
    "Load Test/Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d04a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = \"val\"\n",
    "keypoints_path = f\"./data/person_keypoints_{is_train}2017.json\"\n",
    "img_dir = f'./data/train2017_man_{is_train}'\n",
    "label_path = f'./data/label_coco_man_{is_train}.csv'\n",
    "val_data = InputDataset(keypoints_path,img_dir, label_path)\n",
    "val_loader = DataLoader(val_data, batch_size=64, num_workers=6, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd85dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect() #clears cache bcs sometimes it breaks\n",
    "    model_path = path #can be changed from path in model to other saved models\n",
    "    model.load_state_dict(torch.load(path, weights_only=True))\n",
    "    model = MixedModel().to(DEVICE)\n",
    "    kps, soft_kps = run_test(model, val_loader, pose_criterion, age_criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9825fe",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2623fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eafe2777",
   "metadata": {},
   "source": [
    "## Run on real videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5558d56",
   "metadata": {},
   "source": [
    "The plan is to track people, crop the boxes and detect keypoints+age on the cropped images. The issue is occlusion and people leaving and reappearing in the view.\n",
    "\n",
    "TO DO: permanent id tracking, saving the ids for some time and reid (current version has iding based on ratios which is not memory efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a8e457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "keypoint_names = [\n",
    "\"nose\",\"left_eye\", \"right_eye\", \"left_ear\", \"right_ear\", \"left_shoulder\", \"right_shoulder\",\"left_elbow\", \"right_elbow\",\n",
    "\"left_wrist\", \"right_wrist\", \"left_hip\", \"right_hip\", \"left_knee\", \"right_knee\",\"left_ankle\", \"right_ankle\"\n",
    "]\n",
    "\n",
    "\n",
    "def calc_skeleton(kps, conf, frame_number, person_id)->list:\n",
    "    \"\"\"Calculate skeleton\"\"\"\n",
    "    kp = np.array(kps)\n",
    "    c = np.array(conf)\n",
    "    row = [frame_number,person_id]\n",
    "    for i, (x, y) in enumerate(kp):\n",
    "        row.extend([float(x), float(y),float(c[i])])\n",
    "    return row \n",
    "\n",
    "def gen_boxes(v:str, v_name:str, device:str=DEVICE):\n",
    "    \"\"\"Generate boxes\"\"\"\n",
    "    box_model = YOLO(\"models/yolo11n.pt\")\n",
    "    box_model.to(device)\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(v)\n",
    "    except:\n",
    "        print(\"No video\")\n",
    "        return\n",
    "    \n",
    "    header = [\"frame\", \"person_id\"]\n",
    "    for name in keypoint_names:\n",
    "        header.extend([f\"{name}_x\", f\"{name}_y\",f\"{name}_conf\"])\n",
    "        \n",
    "    rows = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        results = box_model.track(\n",
    "            frame,\n",
    "            classes=[0],\n",
    "            persist=True,\n",
    "            verbose=False,\n",
    "            show=False, \n",
    "            tracker=\"botsort2.yaml\"\n",
    "        )\n",
    "        \n",
    "        result = results[0]\n",
    "        \n",
    "        if (result.boxes is not None and \n",
    "            result.boxes.id is not None):\n",
    "            \n",
    "            tracker_ids = result.boxes.id.int().cpu().tolist()\n",
    "            boxes_xyxy = result.boxes.xyxy.cpu().numpy()\n",
    "            boxes_xywh = result.boxes.xywh.cpu().numpy()\n",
    "            \n",
    "            #if frame_nr % 10:\n",
    "            #    print(f\"frame:{frame_nr}\")\n",
    "\n",
    "    with open(f\"{v_name}_keypoints.csv\", \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b30fae0",
   "metadata": {},
   "source": [
    "### Run prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6684909d",
   "metadata": {},
   "source": [
    "### Display keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9161b657",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2982795",
   "metadata": {},
   "source": [
    "1. [Tutorial on adding another head when predicting](https://y-t-g.github.io/tutorials/yolov8n-add-classes/)\n",
    "2. [Ultralytics model training](https://docs.ultralytics.com/modes/train/#idle-gpu-training)\n",
    "3. [Roboflow tutorial](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-keypoint.ipynb)\n",
    "4. [Keypoints with heatmaps](https://www.slingacademy.com/article/creating-a-keypoint-detection-model-with-pytorch-and-heatmap-regression/)\n",
    "5. [Tutorial on keypoints regression, used some heatmap functions](https://elte.me/2021-03-10-keypoint-regression-fastai)\n",
    "6. [Heatmap transform](https://github.com/baoshengyu/H3R/blob/master/torchalign/heatmap_head/transforms/functional.py)\n",
    "7. [Heatmap regression via randomized rounding](https://github.com/baoshengyu/H3R)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yolo118)",
   "language": "python",
   "name": "yolo118"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
